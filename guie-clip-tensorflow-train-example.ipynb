{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Possible ideas\n",
    "- add rotation of images to the model\n",
    "- use not top 50 classes, but a random range of classes\n",
    "- check to see that we use all of training data as a final round of training\n",
    "- remove magic numbers from training"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "id": "cUF4H1xBsYb6",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "def is_colab_env():\n",
    "    is_colab = False\n",
    "    for k in os.environ.keys():\n",
    "        if \"COLAB\" in k:\n",
    "            is_colab = True\n",
    "            break\n",
    "    return is_colab\n",
    "\n",
    "# if google colab, install transformers and tensorflow_addons\n",
    "# (Note: please use google colab(TPU) when model is trained. \n",
    "#  On the kaggle TPU env, the module transformers.TFCLIPVisionModel couldn't be installed.)\n",
    "if is_colab_env():\n",
    "    # if it is a colab env, install these libs\n",
    "    !pip install transformers\n",
    "    !pip install tensorflow_addons"
   ],
   "metadata": {
    "id": "05oZ1Q5Idhj-",
    "outputId": "548ec03d-91de-4a68-bfb9-9830ea763b5e",
    "execution": {
     "iopub.status.busy": "2022-10-01T18:51:17.130414Z",
     "iopub.execute_input": "2022-10-01T18:51:17.130939Z",
     "iopub.status.idle": "2022-10-01T18:51:17.161620Z",
     "shell.execute_reply.started": "2022-10-01T18:51:17.130862Z",
     "shell.execute_reply": "2022-10-01T18:51:17.160799Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow_hub as tfhub\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import requests\n",
    "from mpl_toolkits import axes_grid1"
   ],
   "metadata": {
    "id": "i72153AaDJds",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Device"
   ],
   "metadata": {
    "id": "k8BdwSO1sYcN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "    # dectect and init TPU - turn on by accelerator switch\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "    \n",
    "except ValueError: # if no TPU\n",
    "    tpu = None\n",
    "\n",
    "# use TPU if availabple\n",
    "if tpu:\n",
    "    print('Running on TPU ', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu) # distribute training among different tpu\n",
    "else:\n",
    "    print(\"using GPU/CPU \", tf.config.list_physical_devices(device_type=None))\n",
    "    # list the available devices\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "id": "khrTPhLcR39a",
    "outputId": "7adb1df9-4f27-4042-c084-cfc7881b8728",
    "execution": {
     "iopub.status.busy": "2022-10-01T19:29:17.374233Z",
     "iopub.execute_input": "2022-10-01T19:29:17.375091Z",
     "iopub.status.idle": "2022-10-01T19:29:22.327215Z",
     "shell.execute_reply.started": "2022-10-01T19:29:17.375052Z",
     "shell.execute_reply": "2022-10-01T19:29:22.325555Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Duplicate registrations for type 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# dectect and init TPU - turn on by accelerator switch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     tpu \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mcluster_resolver\u001B[38;5;241m.\u001B[39mTPUClusterResolver(tpu\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/tensorflow/__init__.py:473\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_current_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    472\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 473\u001B[0m     \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py:41\u001B[0m, in \u001B[0;36mLazyLoader._load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent_module_globals[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_name] \u001B[38;5;241m=\u001B[39m module\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Emit a warning if one was specified\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/importlib/__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/keras/__init__.py:25\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/keras/models/__init__.py:18\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-bad-import-order\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Functional\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/keras/engine/functional.py:25\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base_layer\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base_layer_utils\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m input_layer \u001B[38;5;28;01mas\u001B[39;00m input_layer_module\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/keras/engine/base_layer.py:40\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m node \u001B[38;5;28;01mas\u001B[39;00m node_module\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autocast_variable\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m loss_scale_optimizer\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m policy\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layer_serialization\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py:20\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizers\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m loss_scale \u001B[38;5;28;01mas\u001B[39;00m keras_loss_scale_module\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizer_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizer_v2\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizer_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils \u001B[38;5;28;01mas\u001B[39;00m optimizer_utils\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:1461\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m   1454\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_config\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1455\u001B[0m     \u001B[38;5;66;03m# TODO(allenl): Save and restore the Optimizer's config\u001B[39;00m\n\u001B[1;32m   1456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m   1457\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRestoring functional Optimizers from SavedModels is not currently \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1458\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msupported. Please file a feature request if this limitation bothers \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1459\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1461\u001B[0m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__internal__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaved_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregister_revived_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1462\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1463\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptimizerV2\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mversions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__internal__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaved_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mVersionedTypeRegistration\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobject_factory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mRestoredOptimizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1466\u001B[0m \u001B[43m        \u001B[49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_producer_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1468\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_consumer_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1469\u001B[0m \u001B[43m        \u001B[49m\u001B[43msetter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRestoredOptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_hyper\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m   1470\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/anaconda3/envs/tf2_venv/lib/python3.8/site-packages/tensorflow/python/saved_model/revived_types.py:133\u001B[0m, in \u001B[0;36mregister_revived_type\u001B[0;34m(identifier, predicate, versions)\u001B[0m\n\u001B[1;32m    130\u001B[0m   version_numbers\u001B[38;5;241m.\u001B[39madd(registration\u001B[38;5;241m.\u001B[39mversion)\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m identifier \u001B[38;5;129;01min\u001B[39;00m _REVIVED_TYPE_REGISTRY:\n\u001B[0;32m--> 133\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDuplicate registrations for type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00midentifier\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    135\u001B[0m _REVIVED_TYPE_REGISTRY[identifier] \u001B[38;5;241m=\u001B[39m (predicate, versions)\n\u001B[1;32m    136\u001B[0m _TYPE_IDENTIFIERS\u001B[38;5;241m.\u001B[39mappend(identifier)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Duplicate registrations for type 'optimizer'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clip\n",
    "- train on image, text pairs\n",
    "- zero-shot learning: do well tasks not trained on\n",
    "- bc of this, it can predict classes not seen before\n",
    "- use image embedding: map image to a location in a x-dim vector\n",
    "- has image and text encoding\n",
    "- images be close to the text after encoding both\n",
    "- use cosine similarity\n",
    "- want cosine similarity to be low between things not similar\n",
    "- take long time to train (why preload weights in class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# If GPU instance, it makes mixed precision enable.\n",
    "if strategy.num_replicas_in_sync == 1:\n",
    "\n",
    "    from keras.mixed_precision import experimental as mixed_precision\n",
    "    # use both the 16-bit and 32-bit float vlaues\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)"
   ],
   "metadata": {
    "id": "tKetdL8BsYcQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# a class to hold configuration of training\n",
    "# holds the CLIP model, preloaded weights, randomization seed, number classes, LR, epochs, Batch size\n",
    "class config:\n",
    "    VERSION = 3\n",
    "    SUBV = \"Clip_ViT_Train\" # sub version\n",
    "\n",
    "    SEED = 42 # the randomization seed\n",
    "\n",
    "    # pretrained model\n",
    "    RESUME = True\n",
    "    RESUME_EPOCH = 0\n",
    "    RESUME_WEIGHT = \"../input/guei-v6-clip-vit-large-arcface-train-projection/clip-vit-large-patch14_224pix-emb256_arcface_entire.h5\"\n",
    "\n",
    "    # backbone model\n",
    "    # feature extracting network\n",
    "    model_type = \"clip-vit-large-patch14\"\n",
    "    EFF_SIZE = 0\n",
    "    EFF2_TYPE = \"\"\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    # projection layer\n",
    "    N_CLASSES = 17691 # the number of classifications for the classes\n",
    "    EMB_DIM = 256  # = 64 x N\n",
    "    \n",
    "    # training\n",
    "    TRAIN = False\n",
    "    BATCH_SIZE = 150 * strategy.num_replicas_in_sync\n",
    "    EPOCHS = 150\n",
    "    LR = 0.001\n",
    "    save_dir = \"./\"\n",
    "\n",
    "    DEBUG = False\n",
    "    \n",
    "\n",
    "# Function to seed everything (set randomness)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# Determine the full model name\n",
    "MODEL_NAME = None\n",
    "if config.model_type == 'effnetv1':\n",
    "    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\n",
    "elif config.model_type == 'effnetv2':\n",
    "    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\n",
    "elif \"swin\" in config.model_type:\n",
    "    MODEL_NAME = config.model_type\n",
    "elif \"conv\" in config.model_type:\n",
    "    MODEL_NAME = config.model_type\n",
    "else:\n",
    "    MODEL_NAME = config.model_type\n",
    "config.MODEL_NAME = MODEL_NAME\n",
    "\n",
    "print(MODEL_NAME)"
   ],
   "metadata": {
    "id": "lCwQB_L1NGoH",
    "outputId": "040dbaa6-54e6-4135-8251-bd779314b05f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFRecords\n",
    "- way to store data\n",
    "- stored sequentially - faster read tiems when compared to numpy array\n",
    "-"
   ],
   "metadata": {
    "id": "FaUn2_W4Hm3t",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if is_colab_env(): # for google colab env.\n",
    "    # load in the data sets: products10k glr2021mini imagenet1k\n",
    "    kaggle_backet_dict = {\n",
    "        \"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://kds-33230144c2940b6311609dcfaaa9841a44508cd149b0616d5e5fb5c2\",\n",
    "        \"guie-products10k-tfrecords-label-1000-10690\" : \"gs://kds-2ae72f61d1fe606ae8aa6af28c61cd39530fefb9797fd88946ac6037\",\n",
    "        \"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://kds-f25e426fa1a600c0fb6cdbfddbb34802a11bed80cf42a4c18baa2fb9\",\n",
    "    }\n",
    "else: # for kaggle notebook\n",
    "    from kaggle_datasets import KaggleDatasets"
   ],
   "metadata": {
    "id": "IWZk8JHMdKhG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_num_of_image(file):\n",
    "    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n",
    "\n",
    "train_set_len = sum( [ get_num_of_image(file) for file in train_set_path ] )\n",
    "valid_set_len = sum( [ get_num_of_image(file) for file in valid_set_path ] )\n",
    "\n",
    "train_set_len, valid_set_len"
   ],
   "metadata": {
    "id": "ak_bok57zAZ_",
    "outputId": "846c548b-3e24-4e7c-e7f3-bafc690fea7f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_shard_suffix = '*-train-*.tfrec'\n",
    "\n",
    "ROOT_DIRS  = [\n",
    "    \"guie-glr2021mini-tfrecords-label-10691-17690\",\n",
    "    \"guie-imagenet1k-mini1-tfrecords-label-0-999\",\n",
    "    \"guie-products10k-tfrecords-label-1000-10690\",\n",
    "]\n",
    "\n",
    "train_set_path = []\n",
    "valid_set_path = []\n",
    "for ROOT_DIR in ROOT_DIRS: # for each dataset\n",
    "    if is_colab_env():\n",
    "        GCS_DS_PATH = kaggle_backet_dict[ ROOT_DIR ]\n",
    "    else:\n",
    "        GCS_DS_PATH = KaggleDatasets().get_gcs_path( ROOT_DIR )\n",
    "\n",
    "    print( f\"\\\"{ROOT_DIR}\\\" : \\\"{GCS_DS_PATH}\\\",\" )\n",
    "    files = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix}'))\n",
    "    # split data\n",
    "    # TODO: try tinkering with percent data used in val and train, potentially setting all data to the\n",
    "    train_set_path += random.sample(files, int( len(files) * 0.9 ) ) # add on 90% to training data\n",
    "    valid_set_path += [ file for file in files  if not file in train_set_path ] # add on remaining to training data\n",
    "    print(ROOT_DIR, \", number of tfrecords = \", len(files))\n",
    "\n",
    "# sort the data\n",
    "train_set_path = sorted( train_set_path )\n",
    "valid_set_path = sorted( valid_set_path )\n",
    "\n",
    "print(\"# of tfrecords for training   : \", len(train_set_path))\n",
    "print(\"# of tfrecords for validation : \", len(valid_set_path))\n",
    "\n",
    "# not running full: use less data.\n",
    "if config.DEBUG:\n",
    "    # TODO: move 4 to a variable in the config class\n",
    "    train_set_path = random.sample( train_set_path, 4)\n",
    "    print(\"debug: reduce training data. num=\", len(train_set_path))\n",
    "\n",
    "    valid_set_path = train_set_path #valid_set_path[:1]\n",
    "    print(\"debug: reduce validation data. num=\", len(valid_set_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset pipeline\n",
    "- converts the raw examples (i/o pair) to an image and its label"
   ],
   "metadata": {
    "id": "aZuGi10XOuiW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def deserialization_fn(serialized_example):\n",
    "    # examples stored in TFrecords, pass 1 exmaple to process it\n",
    "    parsed_example = tf.io.parse_single_example( # example contains a image class, and a label for that image\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    )\n",
    "    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE)) # TODO: try changing the image size\n",
    "    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # resize to 0-1\n",
    "    return image, label # return the processed data"
   ],
   "metadata": {
    "id": "hAmYgnXmFE3P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# arcFace: model takes 2 face images as input and outputs the distance between them\n",
    "def arcface_format(image, label_group):\n",
    "    return {'inp1': image, 'inp2': label_group}, label_group\n",
    "# convert image to 0-255 pixel size\n",
    "def rescale_image(image, label_group):\n",
    "    image = tf.cast(image, tf.float32) * 255.0\n",
    "    return image, label_group\n",
    "\n",
    "# Data augmentation function\n",
    "def data_augment(image, label_group):\n",
    "    # TODO: try shifting the image around and see what you get\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return image, label_group\n",
    "\n",
    "# Dataset to obtain backbone's inference\n",
    "def get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths) # represents lasrge dataset, all of the training data\n",
    "    data_len = sum( [ get_num_of_image(file) for file in tfrecord_paths ] )\n",
    "    dataset = dataset.shuffle( data_len//10 ) if shuffle else dataset # shouffle data after each epoch training\n",
    "    dataset = dataset.flat_map(tf.data.TFRecordDataset) # combines different TFRecords for a dataset, have large list of data\n",
    "    #dataset.map: apply the deserialization_fn to each dataset\n",
    "    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n",
    "\n",
    "    if augment:\n",
    "        # apply augment func to each element\n",
    "        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n",
    "    # apply rescale func to each eelement (convert back to 0-255 rgb)\n",
    "    # working with 0-255 as prior to this, flattened to 0-1\n",
    "    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n",
    "    # convert to arcface format (distance between 2 images)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    # init the dataset, spliting the examples into batches\n",
    "    dataset = dataset.batch(config.BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ],
   "metadata": {
    "id": "UDsIA9z0NXIe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Viz tfrecord images"
   ],
   "metadata": {
    "id": "EbjJ86S4sYci",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "backbone_infer_dataset_encode = get_backbone_inference_dataset(train_set_path, shuffle=True, augment=True)\n",
    "# display some images in 15x15 pixel size to see the data being used with the labels\n",
    "num_cols = 3\n",
    "num_rows = 5\n",
    "backbone_infer_dataset_encode = backbone_infer_dataset_encode.unbatch().batch(num_cols * num_rows)\n",
    "x, y = next(iter(backbone_infer_dataset_encode))\n",
    "print(x[\"inp1\"].shape)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "grid = axes_grid1.ImageGrid(fig, 111, nrows_ncols=(num_cols, num_rows), axes_pad=0.1)\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "    ax.imshow(x[\"inp1\"][i]/255)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "del backbone_infer_dataset_encode"
   ],
   "metadata": {
    "id": "QZ92qkVDHHL9",
    "outputId": "fb631f57-c754-4bd6-d742-b88bffc9c057",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "dOPX4LshNXsM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "    - a loss func used in face recognition\n",
    "    - uses cosine similarity\n",
    "    -\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "        # return configuration of model\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # use existing class and pass in the input shape\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "    # only accepts tensors as input (i/o pair)\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32) # the output class\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2)) # trig identify\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output # return the predicted output in embedded space"
   ],
   "metadata": {
    "id": "1LjaDRLgMjdq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_scale_layer(rescale_mode = \"tf\"):\n",
    "    # For keras_cv_attention_models module:\n",
    "    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n",
    "    # ref function : init_mean_std_by_rescale_mode()\n",
    "\n",
    "    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n",
    "\n",
    "    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n",
    "        mean, std = rescale_mode\n",
    "    elif rescale_mode == \"torch\":\n",
    "        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
    "        std = np.array([0.229, 0.224, 0.225]) * 255.0\n",
    "    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n",
    "        mean, std = 127.5, 127.5\n",
    "    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n",
    "        mean, std = 128.0, 128.0\n",
    "    elif rescale_mode == \"raw01\":\n",
    "        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n",
    "    else:\n",
    "        mean, std = 0, 1  # raw inputs [0, 255]        \n",
    "    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n",
    "    # arbitrary expressions: lamda:\n",
    "    # help whe nbuilding sequental and functional API midels\n",
    "    # have the function, and here we nromalize the input vector according to the mean and std of model\n",
    "    \n",
    "    return scaling_layer\n",
    "\n",
    "\n",
    "def get_clip_model():\n",
    "    # define the input shape to be 3 channels each 224 by 224 pixels\n",
    "    inp = tf.keras.layers.Input(shape = [3, 224, 224]) # [B, C, H, W]\n",
    "    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    # the output is the vectorized image\n",
    "    output = backbone({'pixel_values':inp}).pooler_output\n",
    "    return tf.keras.Model(inputs=[inp], outputs=[output])\n",
    "\n",
    "def get_embedding_model():\n",
    "    #------------------\n",
    "    # Definition of placeholders\n",
    "    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "\n",
    "    # Definition of layers\n",
    "    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n",
    "    layer_scaling = get_scale_layer(rescale_mode = \"torch\") # first scale the layers of the image, have different rescale inputs\n",
    "    layer_permute = tf.keras.layers.Permute((3,1,2)) # change the fields form R,G,B to B,R,G\n",
    "    layer_backbone = get_clip_model() # load the CLIp model\n",
    "    layer_dropout = tf.keras.layers.Dropout(0.2) # have dropout (randomly set input nodes to 0 to prevent overfitting\n",
    "    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM) # the input length ie N*256 - the dimensions ie 2*256\n",
    "    layer_margin = ArcMarginProduct(\n",
    "        n_classes = config.N_CLASSES, \n",
    "        s = 30, \n",
    "        m = 0.3, \n",
    "        name=f'head/arcface', \n",
    "        dtype='float32'\n",
    "        )\n",
    "    # activation function\n",
    "    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n",
    "    layer_l2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')\n",
    "    \n",
    "    if config.EMB_DIM != 64:\n",
    "        layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n",
    "    else:\n",
    "        layer_adaptive_pooling = tf.keras.layers.Lambda(lambda x: x )  # layer with no operation\n",
    "\n",
    "    #------------------\n",
    "    # Definition of entire model\n",
    "    image = layer_scaling(inp) # scaling: normalize image according to a mean and standard deviation\n",
    "    image = layer_resize(image) # resize the image to have it be in desired image size 256x256\n",
    "    image = layer_permute(image) # change the channel layout bc BGR training is better than RGB\n",
    "    backbone_output = layer_backbone(image) # the encoded clip model for producing vectorized input and output pairs\n",
    "    embed = layer_dropout(backbone_output) # dropout: randomly deactivate nuerons to prevent overfitting\n",
    "    embed = layer_dense_before_arcface(embed) # the output of the encoding of the vectors\n",
    "    x = layer_margin([embed, label]) # the actual conversion from one to the other via cosine similarity\n",
    "    output = layer_softmax(x) # activation function of the layer_margin\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n",
    "\n",
    "    model.layers[-6].trainable = False\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "        )\n",
    "\n",
    "    #------------------\n",
    "    # Definition of embedding model (for submission)\n",
    "    embed_model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(None, None, 3), dtype='uint8'),\n",
    "        layer_scaling,\n",
    "        layer_resize,\n",
    "        layer_permute,\n",
    "        layer_backbone,\n",
    "        layer_dropout,\n",
    "        layer_dense_before_arcface,\n",
    "        layer_adaptive_pooling,    # shape:[None, config.EMB_DIM] --> [None, 64]\n",
    "        layer_l2,\n",
    "    ])\n",
    "\n",
    "\n",
    "    return model, embed_model"
   ],
   "metadata": {
    "id": "jX76WJYoMgey",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "    model, emb_model = get_embedding_model()\n",
    "\n",
    "if config.RESUME:\n",
    "    print(f\"load {config.RESUME_WEIGHT}\")\n",
    "    model.load_weights( config.RESUME_WEIGHT )\n",
    "    #emb_model.load_weights( config.RESUME_WEIGHT )"
   ],
   "metadata": {
    "id": "5nGM8XncMglt",
    "outputId": "a8526809-ce45-4b10-f642-a72c6014cb1e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "id": "qSWkPesHMgpO",
    "outputId": "b65d4347-69b9-4d35-fcc4-02cf8e1d68b7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "emb_model.summary()"
   ],
   "metadata": {
    "id": "0a5LW0tnMgsX",
    "outputId": "9143889a-8d13-493d-bb8a-f0323a7da97f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scheduler"
   ],
   "metadata": {
    "id": "5z_2vp1TsYcx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# a way to decrease learning rate over time. As epoch increase, decrease the learning rate\n",
    "def get_lr_callback(plot=False):\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * config.BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.95\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if config.RESUME:\n",
    "            epoch = epoch + config.RESUME_EPOCH\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "        \n",
    "    if plot:\n",
    "        epochs = list(range(config.EPOCHS))\n",
    "        learning_rates = [lrfn(x) for x in epochs]\n",
    "        plt.scatter(epochs,learning_rates)\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "get_lr_callback(plot=True)"
   ],
   "metadata": {
    "id": "X091wmn0sYcy",
    "outputId": "a268611b-5af9-46ce-f1e0-f2276ed8e0ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train entire model "
   ],
   "metadata": {
    "id": "CDmefQyfsYcz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if config.TRAIN:\n",
    "    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
    "        config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\", monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='max', save_freq='epoch')\n",
    "\n",
    "    steps_per_epoch = train_set_len // config.BATCH_SIZE  // 10     # \"//10\" means that the lr is update every 0.1 epoch.\n",
    "    validation_steps = valid_set_len // config.BATCH_SIZE\n",
    "    if valid_set_len % config.BATCH_SIZE != 0:\n",
    "        validation_steps += 1\n",
    "    print(steps_per_epoch, validation_steps)\n",
    "    # get the train and validation datasets\n",
    "    ds_train = get_backbone_inference_dataset(train_set_path, shuffle=True, augment=True, repeat=True)\n",
    "    ds_valid = get_backbone_inference_dataset(valid_set_path, shuffle=False, augment=False, repeat=False)\n",
    "    # fit the model using the overidden fucntion and the config paramters\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        epochs=config.EPOCHS,\n",
    "        callbacks=[get_lr_callback(), sv_loss], # the adjustment for the LR over time\n",
    "        steps_per_epoch=steps_per_epoch, # number of adjustments per epoch\n",
    "        validation_data = ds_valid, # validation data\n",
    "        validation_steps = validation_steps, # number of steps for the validation\n",
    "        verbose=1 # engage debugging\n",
    "    )\n",
    "\n",
    "    # load best weight \n",
    "    model.load_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_entire.h5\" )"
   ],
   "metadata": {
    "id": "QSTjDBPzsYc1",
    "outputId": "049a324f-940f-4b05-855b-e49bfad189ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save for debug\n",
    "emb_model.save_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_emb_model.h5\" )"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submission.zip"
   ],
   "metadata": {
    "id": "kG-r1le7SF9F",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "save_locally = tf.saved_model.SaveOptions(\n",
    "    experimental_io_device='/job:localhost'\n",
    ")\n",
    "emb_model.save('./embedding_norm_model', options=save_locally)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('submission.zip','w') as zip:           \n",
    "    zip.write(\n",
    "        './embedding_norm_model/saved_model.pb', \n",
    "        arcname='saved_model.pb'\n",
    "    ) \n",
    "    zip.write(\n",
    "        './embedding_norm_model/variables/variables.data-00000-of-00001', \n",
    "        arcname='variables/variables.data-00000-of-00001'\n",
    "    ) \n",
    "    zip.write(\n",
    "        './embedding_norm_model/variables/variables.index', \n",
    "        arcname='variables/variables.index'\n",
    "    )"
   ],
   "metadata": {
    "id": "_eeqo14RMxEr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}