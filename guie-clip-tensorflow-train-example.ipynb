{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Possible ideas\n",
    "- add rotation of images to the model\n",
    "- "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "id": "cUF4H1xBsYb6",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "def is_colab_env():\n",
    "    is_colab = False\n",
    "    for k in os.environ.keys():\n",
    "        if \"COLAB\" in k:\n",
    "            is_colab = True\n",
    "            break\n",
    "    return is_colab\n",
    "\n",
    "# if google colab, install transformers and tensorflow_addons\n",
    "# (Note: please use google colab(TPU) when model is trained. \n",
    "#  On the kaggle TPU env, the module transformers.TFCLIPVisionModel couldn't be installed.)\n",
    "if is_colab_env():\n",
    "    # if it is a colab env, install these libs\n",
    "    !pip install transformers\n",
    "    !pip install tensorflow_addons"
   ],
   "metadata": {
    "id": "05oZ1Q5Idhj-",
    "outputId": "548ec03d-91de-4a68-bfb9-9830ea763b5e",
    "execution": {
     "iopub.status.busy": "2022-10-01T18:51:17.130414Z",
     "iopub.execute_input": "2022-10-01T18:51:17.130939Z",
     "iopub.status.idle": "2022-10-01T18:51:17.161620Z",
     "shell.execute_reply.started": "2022-10-01T18:51:17.130862Z",
     "shell.execute_reply": "2022-10-01T18:51:17.160799Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow_hub as tfhub\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import requests\n",
    "from mpl_toolkits import axes_grid1"
   ],
   "metadata": {
    "id": "i72153AaDJds",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Device"
   ],
   "metadata": {
    "id": "k8BdwSO1sYcN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "    # dectect and init TPU - turn on by accelerator switch\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "    \n",
    "except ValueError: # if no TPU\n",
    "    tpu = None\n",
    "\n",
    "# use TPU if availabple\n",
    "if tpu:\n",
    "    print('Running on TPU ', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu) # distribute training among different tpu\n",
    "else:\n",
    "    print(\"using GPU/CPU \", tf.config.list_physical_devices(device_type=None))\n",
    "    # list the available devices\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "id": "khrTPhLcR39a",
    "outputId": "7adb1df9-4f27-4042-c084-cfc7881b8728",
    "execution": {
     "iopub.status.busy": "2022-10-01T19:29:17.374233Z",
     "iopub.execute_input": "2022-10-01T19:29:17.375091Z",
     "iopub.status.idle": "2022-10-01T19:29:22.327215Z",
     "shell.execute_reply.started": "2022-10-01T19:29:17.375052Z",
     "shell.execute_reply": "2022-10-01T19:29:22.325555Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# If GPU instance, it makes mixed precision enable.\n",
    "if strategy.num_replicas_in_sync == 1:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy) "
   ],
   "metadata": {
    "id": "tKetdL8BsYcQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class config:\n",
    "    VERSION = 3\n",
    "    SUBV = \"Clip_ViT_Train\"\n",
    "\n",
    "    SEED = 42\n",
    "\n",
    "    # pretrained model\n",
    "    RESUME = True\n",
    "    RESUME_EPOCH = 0\n",
    "    RESUME_WEIGHT = \"../input/guei-v6-clip-vit-large-arcface-train-projection/clip-vit-large-patch14_224pix-emb256_arcface_entire.h5\"\n",
    "\n",
    "    # backbone model\n",
    "    model_type = \"clip-vit-large-patch14\"\n",
    "    EFF_SIZE = 0\n",
    "    EFF2_TYPE = \"\"\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    # projection layer\n",
    "    N_CLASSES = 17691\n",
    "    EMB_DIM = 256  # = 64 x N\n",
    "    \n",
    "    # training\n",
    "    TRAIN = False\n",
    "    BATCH_SIZE = 150 * strategy.num_replicas_in_sync\n",
    "    EPOCHS = 150\n",
    "    LR = 0.001\n",
    "    save_dir = \"./\"\n",
    "\n",
    "    DEBUG = False\n",
    "    \n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# model name\n",
    "MODEL_NAME = None\n",
    "if config.model_type == 'effnetv1':\n",
    "    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\n",
    "elif config.model_type == 'effnetv2':\n",
    "    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\n",
    "elif \"swin\" in config.model_type:\n",
    "    MODEL_NAME = config.model_type\n",
    "elif \"conv\" in config.model_type:\n",
    "    MODEL_NAME = config.model_type\n",
    "else:\n",
    "    MODEL_NAME = config.model_type\n",
    "config.MODEL_NAME = MODEL_NAME\n",
    "print(MODEL_NAME)"
   ],
   "metadata": {
    "id": "lCwQB_L1NGoH",
    "outputId": "040dbaa6-54e6-4135-8251-bd779314b05f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFRecords"
   ],
   "metadata": {
    "id": "FaUn2_W4Hm3t",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if is_colab_env(): # for google colab env.\n",
    "    kaggle_backet_dict = {\n",
    "        \"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://kds-33230144c2940b6311609dcfaaa9841a44508cd149b0616d5e5fb5c2\",\n",
    "        \"guie-products10k-tfrecords-label-1000-10690\" : \"gs://kds-2ae72f61d1fe606ae8aa6af28c61cd39530fefb9797fd88946ac6037\",\n",
    "        \"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://kds-f25e426fa1a600c0fb6cdbfddbb34802a11bed80cf42a4c18baa2fb9\",\n",
    "    }\n",
    "else: # for kaggle notebook\n",
    "    from kaggle_datasets import KaggleDatasets"
   ],
   "metadata": {
    "id": "IWZk8JHMdKhG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_shard_suffix = '*-train-*.tfrec'\n",
    "\n",
    "ROOT_DIRS  = [\n",
    "    \"guie-glr2021mini-tfrecords-label-10691-17690\",\n",
    "    \"guie-imagenet1k-mini1-tfrecords-label-0-999\",\n",
    "    \"guie-products10k-tfrecords-label-1000-10690\",\n",
    "]\n",
    "\n",
    "train_set_path = []\n",
    "valid_set_path = []\n",
    "for ROOT_DIR in ROOT_DIRS:\n",
    "    if is_colab_env():\n",
    "        GCS_DS_PATH = kaggle_backet_dict[ ROOT_DIR ]\n",
    "    else:\n",
    "        GCS_DS_PATH = KaggleDatasets().get_gcs_path( ROOT_DIR )\n",
    "        \n",
    "    print( f\"\\\"{ROOT_DIR}\\\" : \\\"{GCS_DS_PATH}\\\",\" )\n",
    "    files = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix}'))\n",
    "    # split data\n",
    "    train_set_path += random.sample(files, int( len(files) * 0.9 ) )\n",
    "    valid_set_path += [ file for file in files  if not file in train_set_path ]\n",
    "    print(ROOT_DIR, \", number of tfrecords = \", len(files))\n",
    "\n",
    "train_set_path = sorted( train_set_path )\n",
    "valid_set_path = sorted( valid_set_path )\n",
    "\n",
    "print(\"# of tfrecords for training   : \", len(train_set_path))\n",
    "print(\"# of tfrecords for validation : \", len(valid_set_path))\n",
    "\n",
    "if config.DEBUG:\n",
    "    train_set_path = random.sample( train_set_path, 4)\n",
    "    print(\"debug: reduce training data. num=\", len(train_set_path))\n",
    "    \n",
    "    valid_set_path = train_set_path #valid_set_path[:1]\n",
    "    print(\"debug: reduce validation data. num=\", len(valid_set_path))"
   ],
   "metadata": {
    "id": "T7qxi07-Hp_q",
    "outputId": "c72e22a4-1112-49cd-b80b-4627a5162b88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_num_of_image(file):\n",
    "    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n",
    "\n",
    "train_set_len = sum( [ get_num_of_image(file) for file in train_set_path ] )\n",
    "valid_set_len = sum( [ get_num_of_image(file) for file in valid_set_path ] )\n",
    "\n",
    "train_set_len, valid_set_len"
   ],
   "metadata": {
    "id": "ak_bok57zAZ_",
    "outputId": "846c548b-3e24-4e7c-e7f3-bafc690fea7f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset pipeline"
   ],
   "metadata": {
    "id": "aZuGi10XOuiW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def deserialization_fn(serialized_example):\n",
    "    parsed_example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    )\n",
    "    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE))\n",
    "    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label"
   ],
   "metadata": {
    "id": "hAmYgnXmFE3P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def arcface_format(image, label_group):\n",
    "    return {'inp1': image, 'inp2': label_group}, label_group\n",
    "\n",
    "def rescale_image(image, label_group):\n",
    "    image = tf.cast(image, tf.float32) * 255.0\n",
    "    return image, label_group\n",
    "\n",
    "# Data augmentation function\n",
    "def data_augment(image, label_group):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return image, label_group\n",
    "\n",
    "# Dataset to obtain backbone's inference\n",
    "def get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths)\n",
    "    data_len = sum( [ get_num_of_image(file) for file in tfrecord_paths ] )\n",
    "    dataset = dataset.shuffle( data_len//10 ) if shuffle else dataset\n",
    "    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n",
    "    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n",
    "\n",
    "    if augment:\n",
    "        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n",
    "    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(config.BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ],
   "metadata": {
    "id": "UDsIA9z0NXIe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Viz tfrecord images"
   ],
   "metadata": {
    "id": "EbjJ86S4sYci",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "backbone_infer_dataset_encode = get_backbone_inference_dataset(train_set_path, shuffle=True, augment=True)\n",
    "\n",
    "num_cols = 3\n",
    "num_rows = 5\n",
    "backbone_infer_dataset_encode = backbone_infer_dataset_encode.unbatch().batch(num_cols * num_rows)\n",
    "x, y = next(iter(backbone_infer_dataset_encode))\n",
    "print(x[\"inp1\"].shape)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "grid = axes_grid1.ImageGrid(fig, 111, nrows_ncols=(num_cols, num_rows), axes_pad=0.1)\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "    ax.imshow(x[\"inp1\"][i]/255)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "del backbone_infer_dataset_encode"
   ],
   "metadata": {
    "id": "QZ92qkVDHHL9",
    "outputId": "fb631f57-c754-4bd6-d742-b88bffc9c057",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "dOPX4LshNXsM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ],
   "metadata": {
    "id": "1LjaDRLgMjdq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_scale_layer(rescale_mode = \"tf\"):\n",
    "    # For keras_cv_attention_models module:\n",
    "    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n",
    "    # ref function : init_mean_std_by_rescale_mode()\n",
    "\n",
    "    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n",
    "\n",
    "    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n",
    "        mean, std = rescale_mode\n",
    "    elif rescale_mode == \"torch\":\n",
    "        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
    "        std = np.array([0.229, 0.224, 0.225]) * 255.0\n",
    "    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n",
    "        mean, std = 127.5, 127.5\n",
    "    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n",
    "        mean, std = 128.0, 128.0\n",
    "    elif rescale_mode == \"raw01\":\n",
    "        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n",
    "    else:\n",
    "        mean, std = 0, 1  # raw inputs [0, 255]        \n",
    "    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n",
    "    \n",
    "    return scaling_layer\n",
    "\n",
    "\n",
    "def get_clip_model():\n",
    "    inp = tf.keras.layers.Input(shape = [3, 224, 224]) # [B, C, H, W]\n",
    "    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    output = backbone({'pixel_values':inp}).pooler_output\n",
    "    return tf.keras.Model(inputs=[inp], outputs=[output])\n",
    "\n",
    "def get_embedding_model():\n",
    "    #------------------\n",
    "    # Definition of placeholders\n",
    "    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "\n",
    "    # Definition of layers\n",
    "    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n",
    "    layer_scaling = get_scale_layer(rescale_mode = \"torch\")\n",
    "    layer_permute = tf.keras.layers.Permute((3,1,2))\n",
    "    layer_backbone = get_clip_model()\n",
    "    layer_dropout = tf.keras.layers.Dropout(0.2)\n",
    "    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM)\n",
    "    layer_margin = ArcMarginProduct(\n",
    "        n_classes = config.N_CLASSES, \n",
    "        s = 30, \n",
    "        m = 0.3, \n",
    "        name=f'head/arcface', \n",
    "        dtype='float32'\n",
    "        )\n",
    "    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n",
    "    layer_l2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')\n",
    "    \n",
    "    if config.EMB_DIM != 64:\n",
    "        layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n",
    "    else:\n",
    "        layer_adaptive_pooling = tf.keras.layers.Lambda(lambda x: x )  # layer with no operation\n",
    "\n",
    "    #------------------\n",
    "    # Definition of entire model\n",
    "    image = layer_scaling(inp)\n",
    "    image = layer_resize(image)\n",
    "    image = layer_permute(image)\n",
    "    backbone_output = layer_backbone(image)\n",
    "    embed = layer_dropout(backbone_output)\n",
    "    embed = layer_dense_before_arcface(embed)\n",
    "    x = layer_margin([embed, label])\n",
    "    output = layer_softmax(x)\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n",
    "\n",
    "    model.layers[-6].trainable = False\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "        )\n",
    "\n",
    "    #------------------\n",
    "    # Definition of embedding model (for submission)\n",
    "    embed_model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(None, None, 3), dtype='uint8'),\n",
    "        layer_scaling,\n",
    "        layer_resize,\n",
    "        layer_permute,\n",
    "        layer_backbone,\n",
    "        layer_dropout,\n",
    "        layer_dense_before_arcface,\n",
    "        layer_adaptive_pooling,    # shape:[None, config.EMB_DIM] --> [None, 64]\n",
    "        layer_l2,\n",
    "    ])\n",
    "\n",
    "\n",
    "    return model, embed_model"
   ],
   "metadata": {
    "id": "jX76WJYoMgey",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "    model, emb_model = get_embedding_model()\n",
    "\n",
    "if config.RESUME:\n",
    "    print(f\"load {config.RESUME_WEIGHT}\")\n",
    "    model.load_weights( config.RESUME_WEIGHT )\n",
    "    #emb_model.load_weights( config.RESUME_WEIGHT )"
   ],
   "metadata": {
    "id": "5nGM8XncMglt",
    "outputId": "a8526809-ce45-4b10-f642-a72c6014cb1e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "id": "qSWkPesHMgpO",
    "outputId": "b65d4347-69b9-4d35-fcc4-02cf8e1d68b7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "emb_model.summary()"
   ],
   "metadata": {
    "id": "0a5LW0tnMgsX",
    "outputId": "9143889a-8d13-493d-bb8a-f0323a7da97f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scheduler"
   ],
   "metadata": {
    "id": "5z_2vp1TsYcx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_lr_callback(plot=False):\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * config.BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.95\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if config.RESUME:\n",
    "            epoch = epoch + config.RESUME_EPOCH\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "        \n",
    "    if plot:\n",
    "        epochs = list(range(config.EPOCHS))\n",
    "        learning_rates = [lrfn(x) for x in epochs]\n",
    "        plt.scatter(epochs,learning_rates)\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "get_lr_callback(plot=True)"
   ],
   "metadata": {
    "id": "X091wmn0sYcy",
    "outputId": "a268611b-5af9-46ce-f1e0-f2276ed8e0ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train entire model "
   ],
   "metadata": {
    "id": "CDmefQyfsYcz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if config.TRAIN:\n",
    "    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
    "        config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\", monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='max', save_freq='epoch')\n",
    "\n",
    "    steps_per_epoch = train_set_len // config.BATCH_SIZE  // 10     # \"//10\" means that the lr is update every 0.1 epoch.\n",
    "    validation_steps = valid_set_len // config.BATCH_SIZE\n",
    "    if valid_set_len % config.BATCH_SIZE != 0:\n",
    "        validation_steps += 1\n",
    "    print(steps_per_epoch, validation_steps)\n",
    "    ds_train = get_backbone_inference_dataset(train_set_path, shuffle=True, augment=True, repeat=True)\n",
    "    ds_valid = get_backbone_inference_dataset(valid_set_path, shuffle=False, augment=False, repeat=False)\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        epochs=config.EPOCHS,\n",
    "        callbacks=[get_lr_callback(), sv_loss],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data = ds_valid,\n",
    "        validation_steps = validation_steps,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # load best weight \n",
    "    model.load_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_entire.h5\" )"
   ],
   "metadata": {
    "id": "QSTjDBPzsYc1",
    "outputId": "049a324f-940f-4b05-855b-e49bfad189ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save for debug\n",
    "emb_model.save_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_emb_model.h5\" )"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submission.zip"
   ],
   "metadata": {
    "id": "kG-r1le7SF9F",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "save_locally = tf.saved_model.SaveOptions(\n",
    "    experimental_io_device='/job:localhost'\n",
    ")\n",
    "emb_model.save('./embedding_norm_model', options=save_locally)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('submission.zip','w') as zip:           \n",
    "    zip.write(\n",
    "        './embedding_norm_model/saved_model.pb', \n",
    "        arcname='saved_model.pb'\n",
    "    ) \n",
    "    zip.write(\n",
    "        './embedding_norm_model/variables/variables.data-00000-of-00001', \n",
    "        arcname='variables/variables.data-00000-of-00001'\n",
    "    ) \n",
    "    zip.write(\n",
    "        './embedding_norm_model/variables/variables.index', \n",
    "        arcname='variables/variables.index'\n",
    "    )"
   ],
   "metadata": {
    "id": "_eeqo14RMxEr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}