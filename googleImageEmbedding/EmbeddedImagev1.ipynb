{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### I hope that this notebook is helpful to enjoy the competition.\n",
    "\n",
    "## Model\n",
    "- for training:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n",
    "- for inference:  \n",
    "backbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n",
    "\n",
    "Note:  \n",
    "[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \n",
    "The CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \n",
    "If you train it with TPU, I recommend to implement on google colab.\n",
    "\n",
    "## Dataset for training:  \n",
    "- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n",
    "  This dataset was created from imagenet(1k) dataset.  \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n",
    "  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n",
    "  To reduce the dataset size, this dataset has only 50 images per class.  \n",
    "\n",
    "- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n",
    "  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n",
    "  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Possible ideas\n",
    "- add rotation of images to the model\n",
    "- use not top 50 classes, but a random range of classes\n",
    "- check to see that we use all of training data as a final round of training\n",
    "- nice to have: remove magic numbers from training"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "id": "cUF4H1xBsYb6",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# kill os and restart\n",
    "import os \n",
    "!pip install -U -q segmentation-models --user\n",
    "os.kill(os.getpid(), 9)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-08T22:25:13.730750Z",
     "iopub.execute_input": "2022-10-08T22:25:13.731402Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#set keras location\n",
    "import os \n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "def is_colab_env():\n",
    "    is_colab = False\n",
    "    for k in os.environ.keys():\n",
    "        if \"COLAB\" in k:\n",
    "            is_colab = True\n",
    "            break\n",
    "    return is_colab\n",
    "\n",
    "# if google colab, install transformers and tensorflow_addons\n",
    "# (Note: please use google colab(TPU) when model is trained. \n",
    "#  On the kaggle TPU env, the module transformers.TFCLIPVisionModel couldn't be installed.)\n",
    "if is_colab_env():\n",
    "    # if it is a colab env, install these libs\n",
    "    !pip install transformers\n",
    "    !pip install tensorflow_addons"
   ],
   "metadata": {
    "id": "05oZ1Q5Idhj-",
    "outputId": "548ec03d-91de-4a68-bfb9-9830ea763b5e",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:25:46.172764Z",
     "iopub.execute_input": "2022-10-08T22:25:46.173182Z",
     "iopub.status.idle": "2022-10-08T22:25:46.187420Z",
     "shell.execute_reply.started": "2022-10-08T22:25:46.173063Z",
     "shell.execute_reply": "2022-10-08T22:25:46.186349Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall tensorflow -y\n",
    "!pip install tensorflow==2.4\n",
    "!pip uninstall keras -y\n",
    "!pip install keras==2.4\n",
    "!pip install statsmodels --upgrade\n",
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.1\n",
    "\n",
    "\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow==2.10\n",
    "# !pip uninstall keras -y\n",
    "# !pip install keras==2.10\n",
    "# !pip install statsmodels --upgrade\n",
    "# !pip uninstall transformers -y\n",
    "# !pip install transformers==4.22.2\n",
    "# import importlib, pkg_resources, tokenizers\n",
    "# importlib.reload(pkg_resources)\n",
    "# importlib.reload(tokenizers) \n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-08T22:25:48.381726Z",
     "iopub.execute_input": "2022-10-08T22:25:48.382026Z",
     "iopub.status.idle": "2022-10-08T22:27:06.513511Z",
     "shell.execute_reply.started": "2022-10-08T22:25:48.381997Z",
     "shell.execute_reply": "2022-10-08T22:27:06.512448Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found existing installation: tensorflow 2.10.0\nUninstalling tensorflow-2.10.0:\n  Successfully uninstalled tensorflow-2.10.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nCollecting tensorflow==2.4\n  Using cached tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\nCollecting flatbuffers~=1.12.0\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (1.15.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (0.2.0)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001B[K     |████████████████████████████████| 14.8 MB 2.1 MB/s eta 0:00:01    |███▋                            | 1.7 MB 2.1 MB/s eta 0:00:07\n\u001B[?25hRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (2.10.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (3.7.4.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (3.18.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (0.37.0)\nCollecting absl-py~=0.10\n  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n\u001B[K     |████████████████████████████████| 132 kB 48.3 MB/s eta 0:00:01\n\u001B[?25hRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (1.12.1)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (1.6.3)\nCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n\u001B[K     |████████████████████████████████| 462 kB 47.4 MB/s eta 0:00:01\n\u001B[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (3.3.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (2.10.1)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (1.32.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (1.1.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (1.1.2)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4) (0.3.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (2.25.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (3.3.4)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (1.8.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (57.4.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (1.34.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (0.4.5)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (0.6.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4) (2.0.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (4.7.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (2021.5.30)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (1.26.6)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4) (3.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4) (3.5.0)\nInstalling collected packages: numpy, absl-py, tensorflow-estimator, flatbuffers, tensorflow\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 1.2.0\n    Uninstalling absl-py-1.2.0:\n      Successfully uninstalled absl-py-1.2.0\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.10.0\n    Uninstalling tensorflow-estimator-2.10.0:\n      Successfully uninstalled tensorflow-estimator-2.10.0\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 22.9.24\n    Uninstalling flatbuffers-22.9.24:\n      Successfully uninstalled flatbuffers-22.9.24\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncaip-notebooks-serverextension 1.0.0 requires google-cloud-bigquery-storage, which is not installed.\ntensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.3 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.18.0 which is incompatible.\nimbalanced-learn 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\ndatasets 1.12.1 requires huggingface-hub<0.1.0,>=0.0.14, but you have huggingface-hub 0.10.0 which is incompatible.\nallennlp 2.7.0 requires transformers<4.10,>=4.1, but you have transformers 4.22.2 which is incompatible.\u001B[0m\nSuccessfully installed absl-py-0.15.0 flatbuffers-1.12 numpy-1.19.5 tensorflow-2.4.0 tensorflow-estimator-2.4.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nFound existing installation: keras 2.10.0\nUninstalling keras-2.10.0:\n  Successfully uninstalled keras-2.10.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nCollecting keras==2.4\n  Using cached Keras-2.4.0-py2.py3-none-any.whl (170 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.4) (2.10.0)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from keras==2.4) (2.4.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras==2.4) (1.19.5)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras==2.4) (1.7.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.4) (5.4.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (0.37.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (2.10.1)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (2.4.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.6.3)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.32.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.2)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (0.3.3)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (3.3.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (3.7.4.3)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.12)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.15.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (0.2.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (1.12.1)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (0.15.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->keras==2.4) (3.18.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.8.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2.25.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.6.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2.0.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.34.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.3.4)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.4.5)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (57.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (4.7.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.4.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2021.5.30)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.26.6)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.5.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.4.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nRequirement already satisfied: statsmodels in /opt/conda/lib/python3.7/site-packages (0.13.2)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels) (0.5.2)\nRequirement already satisfied: scipy>=1.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels) (1.7.1)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from statsmodels) (1.19.5)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.7/site-packages (from statsmodels) (1.3.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->statsmodels) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25->statsmodels) (2.8.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25->statsmodels) (2021.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels) (1.15.0)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nFound existing installation: transformers 4.22.2\nUninstalling transformers-4.22.2:\n  Successfully uninstalled transformers-4.22.2\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nCollecting transformers==4.1\n  Using cached transformers-4.1.0-py3-none-any.whl (1.5 MB)\nCollecting tokenizers==0.9.4\n  Using cached tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (0.0.45)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (2.25.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (4.62.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (3.0.12)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (2021.8.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==4.1) (1.19.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.1) (2.4.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.1) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.1) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.1) (1.26.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.1) (2021.5.30)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.1) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.1) (8.0.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.1) (1.0.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses->transformers==4.1) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==4.1) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==4.1) (3.5.0)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\nSuccessfully installed tokenizers-0.9.4 transformers-4.1.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install torch==1.7.1\n",
    "#!pip uninstall transformers -y\n",
    "#!pip install transformers -U\n",
    "\n",
    "!pip uninstall huggingface_hub -y\n",
    "!pip install huggingface_hub==0.2.1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-08T22:31:04.103275Z",
     "iopub.execute_input": "2022-10-08T22:31:04.103681Z",
     "iopub.status.idle": "2022-10-08T22:31:16.029870Z",
     "shell.execute_reply.started": "2022-10-08T22:31:04.103641Z",
     "shell.execute_reply": "2022-10-08T22:31:16.028590Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found existing installation: huggingface-hub 0.2.1\nUninstalling huggingface-hub-0.2.1:\n  Successfully uninstalled huggingface-hub-0.2.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\nCollecting huggingface_hub==0.2.1\n  Using cached huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (3.7.4.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (3.0.12)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (4.62.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface_hub==0.2.1) (5.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub==0.2.1) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub==0.2.1) (3.5.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub==0.2.1) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub==0.2.1) (2021.5.30)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub==0.2.1) (1.26.6)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub==0.2.1) (2.10)\nInstalling collected packages: huggingface-hub\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 4.22.2 requires huggingface-hub<1.0,>=0.9.0, but you have huggingface-hub 0.2.1 which is incompatible.\ndatasets 1.12.1 requires huggingface-hub<0.1.0,>=0.0.14, but you have huggingface-hub 0.2.1 which is incompatible.\nallennlp 2.7.0 requires transformers<4.10,>=4.1, but you have transformers 4.22.2 which is incompatible.\u001B[0m\nSuccessfully installed huggingface-hub-0.2.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow_hub as tfhub\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import requests\n",
    "from mpl_toolkits import axes_grid1\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "# from transformers import CLIPProcessor\n",
    "# from transformers import CLIPModel\n",
    "# from transformers import CLIPVisionModel\n",
    "from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor"
   ],
   "metadata": {
    "id": "i72153AaDJds",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:30:58.688719Z",
     "iopub.execute_input": "2022-10-08T22:30:58.689055Z",
     "iopub.status.idle": "2022-10-08T22:30:58.766362Z",
     "shell.execute_reply.started": "2022-10-08T22:30:58.689020Z",
     "shell.execute_reply": "2022-10-08T22:30:58.765171Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_308/2900864198.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCLIPProcessor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;31m# from transformers import CLIPProcessor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;31m# Check the dependencies satisfy the minimal versions required.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdependency_versions_check\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m from .utils import (\n\u001B[1;32m     32\u001B[0m     \u001B[0mOptionalDependencyNotAvailable\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/dependency_versions_check.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mpkg\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"tokenizers\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;31m# must be loaded here, or else tqdm check may fail\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mis_tokenizers_available\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_tokenizers_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'is_tokenizers_available' from 'transformers.utils' (/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py)"
     ],
     "ename": "ImportError",
     "evalue": "cannot import name 'is_tokenizers_available' from 'transformers.utils' (/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py)",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Device"
   ],
   "metadata": {
    "id": "k8BdwSO1sYcN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# use TPU if availabple\n",
    "if tpu:\n",
    "    print('Running on TPU ', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu) # distribute training among different tpu\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "else:\n",
    "    print(\"using GPU/CPU \", tf.config.list_physical_devices(device_type=None))\n",
    "    # list the available devices\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "id": "khrTPhLcR39a",
    "outputId": "7adb1df9-4f27-4042-c084-cfc7881b8728",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:23:07.662130Z",
     "iopub.execute_input": "2022-10-08T22:23:07.662490Z",
     "iopub.status.idle": "2022-10-08T22:23:22.566199Z",
     "shell.execute_reply.started": "2022-10-08T22:23:07.662449Z",
     "shell.execute_reply": "2022-10-08T22:23:22.564828Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Running on TPU  grpc://10.0.0.2:8470\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-10-08 22:23:07.670455: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-10-08 22:23:07.670517: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-10-08 22:23:07.670549: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d24de0494e2e): /proc/driver/nvidia/version does not exist\n2022-10-08 22:23:07.672395: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-10-08 22:23:07.688600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-10-08 22:23:07.688671: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:32933}\n2022-10-08 22:23:07.703633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-10-08 22:23:07.703725: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:32933}\n2022-10-08 22:23:07.704888: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:32933\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Running on TPU  grpc://10.0.0.2:8470\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-10-08 22:23:13.639349: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-10-08 22:23:13.639421: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:32933}\n2022-10-08 22:23:13.641290: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-10-08 22:23:13.641356: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:32933}\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\nREPLICAS:  8\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clip\n",
    "- train on image, text pairs\n",
    "- zero-shot learning: do well tasks not trained on\n",
    "- bc of this, it can predict classes not seen before\n",
    "- use image embedding: map image to a location in a x-dim vector\n",
    "- has image and text encoding\n",
    "- images be close to the text after encoding both\n",
    "- use cosine similarity\n",
    "- want cosine similarity to be low between things not similar\n",
    "- take long time to train (why preload weights in class)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# If GPU instance, it makes mixed precision enable.\n",
    "if strategy.num_replicas_in_sync == 1:\n",
    "\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    # use both the 16-bit and 32-bit float vlaues\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)"
   ],
   "metadata": {
    "id": "tKetdL8BsYcQ",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:23:28.543689Z",
     "iopub.execute_input": "2022-10-08T22:23:28.544802Z",
     "iopub.status.idle": "2022-10-08T22:23:28.550582Z",
     "shell.execute_reply.started": "2022-10-08T22:23:28.544729Z",
     "shell.execute_reply": "2022-10-08T22:23:28.549211Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# a class to hold configuration of training\n",
    "# holds the CLIP model, preloaded weights, randomization seed, number classes, LR, epochs, Batch size\n",
    "class config:\n",
    "    VERSION = 3\n",
    "    SUBV = \"Clip_ViT_Train\" # sub version\n",
    "\n",
    "    SEED = 42 # the randomization seed\n",
    "\n",
    "    # pretrained model\n",
    "    RESUME = True\n",
    "    RESUME_EPOCH = 0\n",
    "    RESUME_WEIGHT = \"../input/guei-v6-clip-vit-large-arcface-train-projection/clip-vit-large-patch14_224pix-emb256_arcface_entire.h5\"\n",
    "\n",
    "    # backbone model\n",
    "    # feature extracting network\n",
    "    model_type = \"clip-vit-large-patch14\"\n",
    "    EFF_SIZE = 0\n",
    "    EFF2_TYPE = \"\"\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    # projection layer\n",
    "    N_CLASSES = 17691 # the number of classifications for the classes\n",
    "    EMB_DIM = 256  # = 64 x N\n",
    "    \n",
    "    # training\n",
    "    TRAIN = True\n",
    "    BATCH_SIZE = 150 * strategy.num_replicas_in_sync\n",
    "    EPOCHS = 150\n",
    "    LR = 0.001\n",
    "    save_dir = \"./\"\n",
    "\n",
    "    DEBUG = False\n",
    "    \n",
    "\n",
    "# Function to seed everything (set randomness)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# Determine the full model name\n",
    "MODEL_NAME = None\n",
    "if config.model_type == 'effnetv1':\n",
    "    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\n",
    "elif config.model_type == 'effnetv2':\n",
    "    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\n",
    "elif \"swin\" in config.model_type:\n",
    "    MODEL_NAME = config.model_type\n",
    "elif \"conv\" in config.model_type:\n",
    "    MODEL_NAME = config.model_type\n",
    "else:\n",
    "    MODEL_NAME = config.model_type\n",
    "config.MODEL_NAME = MODEL_NAME\n",
    "\n",
    "print(MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:23:32.202770Z",
     "iopub.execute_input": "2022-10-08T22:23:32.203136Z",
     "iopub.status.idle": "2022-10-08T22:23:32.213936Z",
     "shell.execute_reply.started": "2022-10-08T22:23:32.203098Z",
     "shell.execute_reply": "2022-10-08T22:23:32.212863Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "clip-vit-large-patch14\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFRecords\n",
    "- way to store data\n",
    "- stored sequentially - faster read tiems when compared to numpy array\n",
    "-"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if is_colab_env(): # for google colab env.\n",
    "    # load in the data sets: products10k glr2021mini imagenet1k\n",
    "    kaggle_backet_dict = {\n",
    "        \"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://kds-33230144c2940b6311609dcfaaa9841a44508cd149b0616d5e5fb5c2\",\n",
    "        \"guie-products10k-tfrecords-label-1000-10690\" : \"gs://kds-2ae72f61d1fe606ae8aa6af28c61cd39530fefb9797fd88946ac6037\",\n",
    "        \"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://kds-f25e426fa1a600c0fb6cdbfddbb34802a11bed80cf42a4c18baa2fb9\",\n",
    "    }\n",
    "else: # for kaggle notebook\n",
    "    from kaggle_datasets import KaggleDatasets"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:00.325971Z",
     "iopub.execute_input": "2022-10-08T22:24:00.326342Z",
     "iopub.status.idle": "2022-10-08T22:24:00.335938Z",
     "shell.execute_reply.started": "2022-10-08T22:24:00.326284Z",
     "shell.execute_reply": "2022-10-08T22:24:00.334947Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_num_of_image(file):\n",
    "    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n",
    "\n",
    "train_set_len = sum( [ get_num_of_image(file) for file in train_set_path ] )\n",
    "valid_set_len = sum( [ get_num_of_image(file) for file in valid_set_path ] )\n",
    "\n",
    "train_set_len, valid_set_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:10.811434Z",
     "iopub.execute_input": "2022-10-08T22:24:10.812545Z",
     "iopub.status.idle": "2022-10-08T22:24:10.824927Z",
     "shell.execute_reply.started": "2022-10-08T22:24:10.812491Z",
     "shell.execute_reply": "2022-10-08T22:24:10.823662Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(478184, 62829)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_shard_suffix = '*-train-*.tfrec'\n",
    "\n",
    "ROOT_DIRS  = [\n",
    "    \"guie-glr2021mini-tfrecords-label-10691-17690\",\n",
    "    \"guie-imagenet1k-mini1-tfrecords-label-0-999\",\n",
    "    \"guie-products10k-tfrecords-label-1000-10690\",\n",
    "]\n",
    "\n",
    "train_set_path = []\n",
    "valid_set_path = []\n",
    "for ROOT_DIR in ROOT_DIRS: # for each dataset\n",
    "    if is_colab_env():\n",
    "        GCS_DS_PATH = kaggle_backet_dict[ ROOT_DIR ]\n",
    "    else:\n",
    "        GCS_DS_PATH = KaggleDatasets().get_gcs_path( ROOT_DIR )\n",
    "\n",
    "    print( f\"\\\"{ROOT_DIR}\\\" : \\\"{GCS_DS_PATH}\\\",\" )\n",
    "    files = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix}'))\n",
    "    # split data\n",
    "    # TODO: try tinkering with percent data used in val and train, potentially setting all data to the\n",
    "    train_set_path += random.sample(files, int( len(files) * 0.9 ) ) # add on 90% to training data\n",
    "    valid_set_path += [ file for file in files  if not file in train_set_path ] # add on remaining to training data\n",
    "    print(ROOT_DIR, \", number of tfrecords = \", len(files))\n",
    "\n",
    "# sort the data\n",
    "train_set_path = sorted( train_set_path )\n",
    "valid_set_path = sorted( valid_set_path )\n",
    "\n",
    "print(\"# of tfrecords for training   : \", len(train_set_path))\n",
    "print(\"# of tfrecords for validation : \", len(valid_set_path))\n",
    "\n",
    "# not running full: use less data.\n",
    "if config.DEBUG:\n",
    "    # TODO: move 4 to a variable in the config class\n",
    "    train_set_path = random.sample( train_set_path, 4)\n",
    "    print(\"debug: reduce training data. num=\", len(train_set_path))\n",
    "\n",
    "    valid_set_path = train_set_path #valid_set_path[:1]\n",
    "    print(\"debug: reduce validation data. num=\", len(valid_set_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:07.244106Z",
     "iopub.execute_input": "2022-10-08T22:24:07.244807Z",
     "iopub.status.idle": "2022-10-08T22:24:08.710103Z",
     "shell.execute_reply.started": "2022-10-08T22:24:07.244753Z",
     "shell.execute_reply": "2022-10-08T22:24:08.709131Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "\"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://kds-03606e46c8d65af026ae8abe2eeea0da71ea563307b7f2c7cd8fcdfa\",\nguie-glr2021mini-tfrecords-label-10691-17690 , number of tfrecords =  32\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-10-08 22:24:07.664118: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://kds-bae63143be1e09373025431718d78649261076d204ecf1a7037aa648\",\nguie-imagenet1k-mini1-tfrecords-label-0-999 , number of tfrecords =  50\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-10-08 22:24:08.250194: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\"guie-products10k-tfrecords-label-1000-10690\" : \"gs://kds-b56ceddd7399f872aa1ac5b4f36a7d37941103259aaf517ad7e86430\",\nguie-products10k-tfrecords-label-1000-10690 , number of tfrecords =  20\n# of tfrecords for training   :  91\n# of tfrecords for validation :  11\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2022-10-08 22:24:08.629638: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset pipeline\n",
    "- converts the raw examples (i/o pair) to an image and its label"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def deserialization_fn(serialized_example):\n",
    "    # examples stored in TFrecords, pass 1 exmaple to process it\n",
    "    parsed_example = tf.io.parse_single_example( # example contains a image class, and a label for that image\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    )\n",
    "    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE)) # TODO: try changing the image size\n",
    "    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # resize to 0-1\n",
    "    return image, label # return the processed data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:15.121392Z",
     "iopub.execute_input": "2022-10-08T22:24:15.121725Z",
     "iopub.status.idle": "2022-10-08T22:24:15.130400Z",
     "shell.execute_reply.started": "2022-10-08T22:24:15.121693Z",
     "shell.execute_reply": "2022-10-08T22:24:15.129119Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Augmentation:\n",
    "    MIN_ROT=-0.261799\n",
    "    MAX_ROT=0.261799\n",
    "    ROTATE=True\n",
    "    HUE=0.1\n",
    "    MIN_SAT=0.7\n",
    "    MAX_SAT=1.3\n",
    "    MIN_CONT=0.80\n",
    "    MAX_CONT=1.20\n",
    "    BRIGHT=0.10\n",
    "    FLIP_LR=True\n",
    "    FLIP_UD=True\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:17.117800Z",
     "iopub.execute_input": "2022-10-08T22:24:17.118130Z",
     "iopub.status.idle": "2022-10-08T22:24:17.124423Z",
     "shell.execute_reply.started": "2022-10-08T22:24:17.118095Z",
     "shell.execute_reply": "2022-10-08T22:24:17.123341Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# arcFace: model takes 2 face images as input and outputs the distance between them\n",
    "def arcface_format(image, label_group):\n",
    "    return {'inp1': image, 'inp2': label_group}, label_group\n",
    "# convert image to 0-255 pixel size\n",
    "def rescale_image(image, label_group):\n",
    "    image = tf.cast(image, tf.float32) * 255.0\n",
    "    return image, label_group\n",
    "    \n",
    "\n",
    "def data_augment(image, label_group):\n",
    "    # TODO: try shifting the image around and see what you get\n",
    "    print(tf.shape(image))\n",
    "    print(tf.shape(label_group))\n",
    "    if Augmentation.FLIP_UD:\n",
    "        image = tf.image.random_flip_up_down(image,seed=config.SEED)\n",
    "    #image = tf.image.random_flip_up_down(image)\n",
    "    if Augmentation.FLIP_LR:\n",
    "        image = tf.image.random_flip_left_right(image,seed=config.SEED)\n",
    "\n",
    "    if Augmentation.ROTATE:\n",
    "        num_samples = int(tf.shape(image))\n",
    "\n",
    "        degrees=tf.random.uniform([], minval=Augmentation.MIN_ROT, maxval=Augmentation.MAX_ROT, dtype=tf.dtypes.float32, seed=2)\n",
    "        #degrees = np.random.uniform(low=Augmentation.MIN_ROT, high=Augmentation.MAX_ROT,size=1)[0]\n",
    "#        print(degrees)\n",
    "#        degrees = tf.random.uniform(minval=Augmentation.MIN_ROT, maxval=Augmentation.MAX_ROT,seed=config.SEED)\n",
    "        #degrees = degrees*0.017453292519943295\n",
    " #       print(degrees)\n",
    "        image = tfa.image.rotate(image, degrees)\n",
    "    image = tf.image.random_hue(image, Augmentation.HUE,seed=config.SEED)\n",
    "    image = tf.image.random_saturation(image, Augmentation.MIN_SAT, Augmentation.MAX_SAT,seed=config.SEED)\n",
    "    image = tf.image.random_contrast(image, Augmentation.MIN_CONT, Augmentation.MAX_CONT,seed=config.SEED)\n",
    "    image = tf.image.random_brightness(image, Augmentation.BRIGHT,seed=config.SEED)\n",
    "\n",
    "    \n",
    "    \n",
    "    return image, label_group\n",
    "\n",
    "# Dataset to obtain backbone's inference\n",
    "def get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths) # represents lasrge dataset, all of the training data\n",
    "    data_len = sum( [ get_num_of_image(file) for file in tfrecord_paths ] )\n",
    "    dataset = dataset.shuffle( data_len//10 ) if shuffle else dataset # shouffle data after each epoch training\n",
    "    dataset = dataset.flat_map(tf.data.TFRecordDataset) # combines different TFRecords for a dataset, have large list of data\n",
    "    #dataset.map: apply the deserialization_fn to each i/o pair in a dataset\n",
    "    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n",
    "\n",
    "    if augment:\n",
    "        # apply augment func to each element\n",
    "#        ds = ds.map(lambda x: fun(x, my_arg))lambda x: fun(x, my_arg)\n",
    "        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n",
    "#        dataset = dataset.map(lambda x: data_augment(x, data_len), num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n",
    "    # apply rescale func to each eelement (convert back to 0-255 rgb)\n",
    "    # working with 0-255 as prior to this, flattened to 0-1\n",
    "    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n",
    "    # convert to arcface format (distance between 2 images)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    # init the dataset, spliting the examples into batches\n",
    "    dataset = dataset.batch(config.BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:34.388964Z",
     "iopub.execute_input": "2022-10-08T22:24:34.389947Z",
     "iopub.status.idle": "2022-10-08T22:24:34.405149Z",
     "shell.execute_reply.started": "2022-10-08T22:24:34.389905Z",
     "shell.execute_reply": "2022-10-08T22:24:34.403685Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Viz tfrecord images"
   ],
   "metadata": {
    "id": "EbjJ86S4sYci",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "backbone_infer_dataset_encode = get_backbone_inference_dataset(train_set_path, shuffle=True, augment=True)\n",
    "\n",
    "# display some images in 15x15 pixel size to see the data being used with the labels\n",
    "num_cols = 3\n",
    "num_rows = 5\n",
    "backbone_infer_dataset_encode = backbone_infer_dataset_encode.unbatch().batch(num_cols * num_rows)\n",
    "x, y = next(iter(backbone_infer_dataset_encode))\n",
    "print(x[\"inp1\"].shape)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "grid = axes_grid1.ImageGrid(fig, 111, nrows_ncols=(num_cols, num_rows), axes_pad=0.1)\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "    ax.imshow(x[\"inp1\"][i]/255)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "del backbone_infer_dataset_encode"
   ],
   "metadata": {
    "id": "QZ92qkVDHHL9",
    "outputId": "fb631f57-c754-4bd6-d742-b88bffc9c057",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:24:39.541665Z",
     "iopub.execute_input": "2022-10-08T22:24:39.542033Z",
     "iopub.status.idle": "2022-10-08T22:24:41.201015Z",
     "shell.execute_reply.started": "2022-10-08T22:24:39.541989Z",
     "shell.execute_reply": "2022-10-08T22:24:41.199654Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Tensor(\"Shape:0\", shape=(3,), dtype=int32)\nTensor(\"Shape_1:0\", shape=(0,), dtype=int32)\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_165/714483081.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mnum_rows\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mbackbone_infer_dataset_encode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbackbone_infer_dataset_encode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_cols\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mnum_rows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbackbone_infer_dataset_encode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"inp1\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    764\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    765\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 766\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_internal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    767\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOutOfRangeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    768\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001B[0m in \u001B[0;36m_next_internal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    746\u001B[0m     \u001B[0;31m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    747\u001B[0m     \u001B[0;31m# to communicate that there is no more data to iterate over.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 748\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecution_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSYNC\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    749\u001B[0m       ret = gen_dataset_ops.iterator_get_next(\n\u001B[1;32m    750\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/contextlib.py\u001B[0m in \u001B[0;36m__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mdel\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 112\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    113\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"generator didn't yield\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001B[0m in \u001B[0;36mexecution_mode\u001B[0;34m(mode)\u001B[0m\n\u001B[1;32m   2480\u001B[0m     \u001B[0mexecutor_old\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecutor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2481\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2482\u001B[0;31m       \u001B[0mexecutor_old\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2483\u001B[0m       \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecutor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexecutor_new\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2484\u001B[0m       \u001B[0;32myield\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     63\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m     \u001B[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m     \u001B[0mpywrap_tfe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTFE_ExecutorWaitForAllPendingNodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mclear_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNotFoundError\u001B[0m: 'AnonymousIteratorV3' is neither a type of a primitive operation nor a name of a function registered in binary running on n-bd11bdee-w-0. Make sure the operation or function is registered in the binary running in this process."
     ],
     "ename": "NotFoundError",
     "evalue": "'AnonymousIteratorV3' is neither a type of a primitive operation nor a name of a function registered in binary running on n-bd11bdee-w-0. Make sure the operation or function is registered in the binary running in this process.",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "dOPX4LshNXsM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "    - a loss func used in face recognition\n",
    "    - uses cosine similarity\n",
    "    -\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "        # return configuration of model\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # use existing class and pass in the input shape\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "    # only accepts tensors as input (i/o pair)\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32) # the output class\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2)) # trig identify\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output # return the predicted output in embedded space"
   ],
   "metadata": {
    "id": "1LjaDRLgMjdq",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:03:42.778361Z",
     "iopub.execute_input": "2022-10-08T22:03:42.778782Z",
     "iopub.status.idle": "2022-10-08T22:03:42.794377Z",
     "shell.execute_reply.started": "2022-10-08T22:03:42.778746Z",
     "shell.execute_reply": "2022-10-08T22:03:42.793453Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_scale_layer(rescale_mode = \"tf\"):\n",
    "    # For keras_cv_attention_models module:\n",
    "    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n",
    "    # ref function : init_mean_std_by_rescale_mode()\n",
    "\n",
    "    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n",
    "\n",
    "    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n",
    "        mean, std = rescale_mode\n",
    "    elif rescale_mode == \"torch\":\n",
    "        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
    "        std = np.array([0.229, 0.224, 0.225]) * 255.0\n",
    "    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n",
    "        mean, std = 127.5, 127.5\n",
    "    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n",
    "        mean, std = 128.0, 128.0\n",
    "    elif rescale_mode == \"raw01\":\n",
    "        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n",
    "    else:\n",
    "        mean, std = 0, 1  # raw inputs [0, 255]        \n",
    "    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n",
    "    # arbitrary expressions: lamda:\n",
    "    # help whe nbuilding sequental and functional API midels\n",
    "    # have the function, and here we nromalize the input vector according to the mean and std of model\n",
    "    \n",
    "    return scaling_layer\n",
    "\n",
    "\n",
    "def get_clip_model():\n",
    "    # define the input shape to be 3 channels each 224 by 224 pixels\n",
    "    inp = tf.keras.layers.Input(shape = [3, 224, 224]) # [B, C, H, W]\n",
    "    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    # the output is the vectorized image\n",
    "    output = backbone({'pixel_values':inp}).pooler_output\n",
    "    return tf.keras.Model(inputs=[inp], outputs=[output])\n",
    "\n",
    "def get_embedding_model():\n",
    "    #------------------\n",
    "    # Definition of placeholders\n",
    "    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "\n",
    "    # Definition of layers\n",
    "    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n",
    "    layer_scaling = get_scale_layer(rescale_mode = \"torch\") # first scale the layers of the image, have different rescale inputs\n",
    "    layer_permute = tf.keras.layers.Permute((3,1,2)) # change the fields form R,G,B to B,R,G\n",
    "    layer_backbone = get_clip_model() # load the CLIp model\n",
    "    layer_dropout = tf.keras.layers.Dropout(0.2) # have dropout (randomly set input nodes to 0 to prevent overfitting\n",
    "    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM) # the input length ie N*256 - the dimensions ie 2*256\n",
    "    layer_margin = ArcMarginProduct(\n",
    "        n_classes = config.N_CLASSES, \n",
    "        s = 30, \n",
    "        m = 0.3, \n",
    "        name=f'head/arcface', \n",
    "        dtype='float32'\n",
    "        )\n",
    "    # activation function\n",
    "    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n",
    "    layer_l2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')\n",
    "    \n",
    "    if config.EMB_DIM != 64:\n",
    "        layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n",
    "    else:\n",
    "        layer_adaptive_pooling = tf.keras.layers.Lambda(lambda x: x )  # layer with no operation\n",
    "\n",
    "    #------------------\n",
    "    # Definition of entire model\n",
    "    image = layer_scaling(inp) # scaling: normalize image according to a mean and standard deviation\n",
    "    image = layer_resize(image) # resize the image to have it be in desired image size 256x256\n",
    "    image = layer_permute(image) # change the channel layout bc BGR training is better than RGB\n",
    "    backbone_output = layer_backbone(image) # the encoded clip model for producing vectorized input and output pairs\n",
    "    embed = layer_dropout(backbone_output) # dropout: randomly deactivate nuerons to prevent overfitting\n",
    "    embed = layer_dense_before_arcface(embed) # the output of the encoding of the vectors\n",
    "    x = layer_margin([embed, label]) # the actual conversion from one to the other via cosine similarity\n",
    "    output = layer_softmax(x) # activation function of the layer_margin\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n",
    "\n",
    "    model.layers[-6].trainable = False\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "        )\n",
    "\n",
    "    #------------------\n",
    "    # Definition of embedding model (for submission)\n",
    "    embed_model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(None, None, 3), dtype='uint8'),\n",
    "        layer_scaling,\n",
    "        layer_resize,\n",
    "        layer_permute,\n",
    "        layer_backbone,\n",
    "        layer_dropout,\n",
    "        layer_dense_before_arcface,\n",
    "        layer_adaptive_pooling,    # shape:[None, config.EMB_DIM] --> [None, 64]\n",
    "        layer_l2,\n",
    "    ])\n",
    "\n",
    "\n",
    "    return model, embed_model"
   ],
   "metadata": {
    "id": "jX76WJYoMgey",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:03:48.012503Z",
     "iopub.execute_input": "2022-10-08T22:03:48.012891Z",
     "iopub.status.idle": "2022-10-08T22:03:48.033435Z",
     "shell.execute_reply.started": "2022-10-08T22:03:48.012860Z",
     "shell.execute_reply": "2022-10-08T22:03:48.032295Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "    model, emb_model = get_embedding_model()\n",
    "\n",
    "if config.RESUME:\n",
    "    print(f\"load {config.RESUME_WEIGHT}\")\n",
    "    model.load_weights( config.RESUME_WEIGHT )\n",
    "    #emb_model.load_weights( config.RESUME_WEIGHT )"
   ],
   "metadata": {
    "id": "5nGM8XncMglt",
    "outputId": "a8526809-ce45-4b10-f642-a72c6014cb1e",
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2022-10-08T22:03:52.219674Z",
     "iopub.execute_input": "2022-10-08T22:03:52.220410Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/4.41k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd16dc20576c4ed38a36f226ea66cfda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.59G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbf578a95bce491b988df90d07b5cf6e"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "id": "qSWkPesHMgpO",
    "outputId": "b65d4347-69b9-4d35-fcc4-02cf8e1d68b7",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "emb_model.summary()"
   ],
   "metadata": {
    "id": "0a5LW0tnMgsX",
    "outputId": "9143889a-8d13-493d-bb8a-f0323a7da97f",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scheduler"
   ],
   "metadata": {
    "id": "5z_2vp1TsYcx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# a way to decrease learning rate over time. As epoch increase, decrease the learning rate\n",
    "def get_lr_callback(plot=False):\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * config.BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.95\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if config.RESUME:\n",
    "            epoch = epoch + config.RESUME_EPOCH\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "        \n",
    "    if plot:\n",
    "        epochs = list(range(config.EPOCHS))\n",
    "        learning_rates = [lrfn(x) for x in epochs]\n",
    "        plt.scatter(epochs,learning_rates)\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "get_lr_callback(plot=True)"
   ],
   "metadata": {
    "id": "X091wmn0sYcy",
    "outputId": "a268611b-5af9-46ce-f1e0-f2276ed8e0ff",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train entire model "
   ],
   "metadata": {
    "id": "CDmefQyfsYcz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if config.TRAIN:\n",
    "    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
    "        config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\", monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='max', save_freq='epoch')\n",
    "\n",
    "    steps_per_epoch = train_set_len // config.BATCH_SIZE  // 10     # \"//10\" means that the lr is update every 0.1 epoch.\n",
    "    validation_steps = valid_set_len // config.BATCH_SIZE\n",
    "    if valid_set_len % config.BATCH_SIZE != 0:\n",
    "        validation_steps += 1\n",
    "    print(steps_per_epoch, validation_steps)\n",
    "    # get the train and validation datasets\n",
    "    ds_train = get_backbone_inference_dataset(train_set_path, shuffle=True, augment=True, repeat=True)\n",
    "    ds_valid = get_backbone_inference_dataset(valid_set_path, shuffle=False, augment=False, repeat=False)\n",
    "    # fit the model using the overidden fucntion and the config paramters\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        epochs=config.EPOCHS,\n",
    "        callbacks=[get_lr_callback(), sv_loss], # the adjustment for the LR over time\n",
    "        steps_per_epoch=steps_per_epoch, # number of adjustments per epoch\n",
    "        validation_data = ds_valid, # validation data\n",
    "        validation_steps = validation_steps, # number of steps for the validation\n",
    "        verbose=1 # engage debugging\n",
    "    )\n",
    "\n",
    "    # load best weight \n",
    "    model.load_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_entire.h5\" )"
   ],
   "metadata": {
    "id": "QSTjDBPzsYc1",
    "outputId": "049a324f-940f-4b05-855b-e49bfad189ff",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save for debug\n",
    "emb_model.save_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_emb_model.h5\" )"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submission.zip"
   ],
   "metadata": {
    "id": "kG-r1le7SF9F",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "save_locally = tf.saved_model.SaveOptions(\n",
    "    experimental_io_device='/job:localhost'\n",
    ")\n",
    "emb_model.save('./embedding_norm_model', options=save_locally)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('submission.zip','w') as zip:           \n",
    "    zip.write(\n",
    "        './embedding_norm_model/saved_model.pb', \n",
    "        arcname='saved_model.pb'\n",
    "    ) \n",
    "    zip.write(\n",
    "        './embedding_norm_model/variables/variables.data-00000-of-00001', \n",
    "        arcname='variables/variables.data-00000-of-00001'\n",
    "    ) \n",
    "    zip.write(\n",
    "        './embedding_norm_model/variables/variables.index', \n",
    "        arcname='variables/variables.index'\n",
    "    )"
   ],
   "metadata": {
    "id": "_eeqo14RMxEr",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}